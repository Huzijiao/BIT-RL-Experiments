{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Policy Gradient Learning with Cart Pole V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\") # declare the environment\n",
    "# Watch the simulation\n",
    "env.reset() # initialize the environment\n",
    "rewards = []\n",
    "for _ in range(100):\n",
    "    env.render() \n",
    "    # Redraw a frame of the environment, default mode means poping up a window, can also set (mode=‘human’, close=False)\n",
    "    # Take a random action, step means advance one time step\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) # state is also known as observation\n",
    "env.close() # Close the environment and clear the memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_size = 4  # 4 informations given by state\n",
    "action_size = 2  # 2 actions possible: left / right\n",
    "hidden_size = 64  # Hidden neurons\n",
    "\n",
    "learning_rate = 0.001 \n",
    "gamma = 0.99 #Discount rate\n",
    "\n",
    "train_episodes = 3000 # An episode is a game\n",
    "max_steps = 900 # Max steps per episode\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build Deep Neural Network\n",
    "class PGAgent():\n",
    "    def __init__(self, input_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Make the NN\n",
    "        # 在神经网络构建graph的时候在模型中的占位,没有把要输入的数据传入模型，只会分配必要的内存\n",
    "        self.inputs = tf.placeholder(tf.float32, \n",
    "                      shape = [None, input_size])\n",
    "                              \n",
    "        # Using ELU is much better than using ReLU\n",
    "        self.hidden_layer_1 = tf.contrib.layers.fully_connected(inputs = self.inputs,\n",
    "                                                  num_outputs = hidden_size,\n",
    "                                                  activation_fn = tf.nn.elu,\n",
    "                                                  weights_initializer = tf.random_normal_initializer())\n",
    "\n",
    "        self.output_layer = tf.contrib.layers.fully_connected(inputs = self.hidden_layer_1,\n",
    "                                                         num_outputs = action_size,\n",
    "                                                 activation_fn = tf.nn.softmax)\n",
    "        \n",
    "        # Log prob output\n",
    "        self.output_log_prob = tf.log(self.output_layer)\n",
    "\n",
    "        # LOSS Function : feed the reward and chosen action in the DNN\n",
    "        self.actions = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.rewards = tf.placeholder(tf.float32, shape = [None])\n",
    "        \n",
    "        # Get log probability of actions from episode : \n",
    "        self.indices = tf.range(0, tf.shape(self.output_log_prob)[0]) * tf.shape(self.output_log_prob)[1] + self.actions\n",
    "        \n",
    "        self.actions_probability = tf.gather(tf.reshape(self.output_layer, [-1]), self.indices)\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(tf.log(self.actions_probability) * self.rewards)\n",
    "\n",
    "        #  Collect some gradients after some training episodes outside the graph and then apply them.\n",
    "  \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32, name=str(idx)+ '_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        \n",
    "        # OPTIMIZER\n",
    "        # 相比于AdaGrad的历史梯度,RMSProp增加了一个衰减系数来控制历史信息的获取多少\n",
    "        # Better to use RMSProp\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weight rewards differently : weight immediate rewards higher than delayed reward\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # Init discount reward matrix\n",
    "    discounted_reward= np.zeros_like(r) \n",
    "    \n",
    "    # Running_add: store sum of reward\n",
    "    running_add = 0\n",
    "    \n",
    "    # Foreach rewards\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        \n",
    "        running_add = running_add * gamma + r[t] # sum * y (gamma) + reward\n",
    "        discounted_reward[t] = running_add\n",
    "    return discounted_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/902805608.py:3: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/2480854332.py:11: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A89F8B50C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A89F8B50C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A89F8B50C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A89F8B50C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A8A20EAFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A8A20EAFC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A8A20EAFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002A8A20EAFC8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/2480854332.py:25: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/2480854332.py:45: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/2480854332.py:57: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/902805608.py:8: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\13521\\AppData\\Local\\Temp/ipykernel_38124/902805608.py:9: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\13521\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 10.0\n",
      "Episode: 100 Total reward: 10.38\n",
      "Episode: 200 Total reward: 10.38\n",
      "Episode: 300 Total reward: 10.37\n",
      "Episode: 400 Total reward: 9.87\n",
      "Episode: 500 Total reward: 10.12\n",
      "Episode: 600 Total reward: 10.17\n",
      "Episode: 700 Total reward: 9.66\n",
      "Episode: 800 Total reward: 9.62\n",
      "Episode: 900 Total reward: 9.82\n",
      "Episode: 1000 Total reward: 9.57\n",
      "Episode: 1100 Total reward: 9.69\n",
      "Episode: 1200 Total reward: 9.79\n",
      "Episode: 1300 Total reward: 10.02\n",
      "Episode: 1400 Total reward: 9.98\n",
      "Episode: 1500 Total reward: 10.22\n",
      "Episode: 1600 Total reward: 9.94\n",
      "Episode: 1700 Total reward: 9.82\n",
      "Episode: 1800 Total reward: 9.71\n",
      "Episode: 1900 Total reward: 9.78\n",
      "Episode: 2000 Total reward: 9.75\n",
      "Episode: 2100 Total reward: 9.91\n",
      "Episode: 2200 Total reward: 9.8\n",
      "Episode: 2300 Total reward: 10.0\n",
      "Episode: 2400 Total reward: 9.82\n",
      "Episode: 2500 Total reward: 9.9\n",
      "Episode: 2600 Total reward: 9.71\n",
      "Episode: 2700 Total reward: 9.64\n",
      "Episode: 2800 Total reward: 9.58\n",
      "Episode: 2900 Total reward: 9.65\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "# Clear the graph\n",
    "tf.reset_default_graph()\n",
    "agent = PGAgent(input_size, action_size, hidden_size, learning_rate, gamma)\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    nb_episodes = 0\n",
    "    \n",
    "    # Define total_rewards and total_length\n",
    "    total_reward = []\n",
    "    total_length = []\n",
    "    \n",
    "    # Not my implementation: \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    \n",
    "    # While we have episodes to train\n",
    "    while nb_episodes < train_episodes:\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        episode_history = [] # Init the array that keep track the history in an episode\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Probabilistically pick an action given our network outputs.\n",
    "            # 建立session，在会话中，运行模型的时候通过向占位符喂入数据\n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action)\n",
    "            \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Append this step in the history of the episode\n",
    "            episode_history.append([state, action, reward, state_1])\n",
    "            \n",
    "            # Now we are in this state (state is now state 1)\n",
    "            state = state_1\n",
    "            \n",
    "            running_reward += reward\n",
    "            \n",
    "            if done == True:\n",
    "                # Update the network\n",
    "                episode_history = np.array(episode_history)\n",
    "                episode_history[:,2] = discount_rewards(episode_history[:,2])\n",
    "                feed_dict={agent.rewards:episode_history[:,2],\n",
    "                        agent.actions:episode_history[:,1],agent.inputs:np.vstack(episode_history[:,0])}\n",
    "                grads = sess.run(agent.gradients, feed_dict=feed_dict)\n",
    "                \n",
    "                \n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if nb_episodes % batch_size == 0 and nb_episodes != 0:\n",
    "                    feed_dict= dictionary = dict(zip(agent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(agent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                #(running_reward))\n",
    "                total_reward.append(running_reward)\n",
    "                total_length.append(step)\n",
    "                break\n",
    "                \n",
    "        # For each 100 episodes\n",
    "        if nb_episodes % 100 == 0:\n",
    "            print(\"Episode: {}\".format(nb_episodes),\n",
    "                    \"Total reward: {}\".format(np.mean(total_reward[-100:])))\n",
    "        nb_episodes += 1\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/cartPoleGame.ckpt\")\n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartPoleGame.ckpt\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 60\n",
    "test_max_steps = 400\n",
    "env.reset()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    for episode in range(1, test_episodes):\n",
    "        t = 0\n",
    "        while t < test_max_steps:\n",
    "            env.render() \n",
    "            #Probabilistically pick an action given our network outputs\n",
    "            action_distribution = sess.run(agent.output_layer ,feed_dict={agent.inputs:[state]})\n",
    "            action = np.random.choice(action_distribution[0],p=action_distribution[0])\n",
    "            action = np.argmax(action_distribution == action) \n",
    "            state_1, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                t = test_max_steps\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, info = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                state = state_1 # Next state\n",
    "                t += 1\n",
    "                \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a3f677a77b69d4794d31f44cefe23a57cb8094311290e7ebc8599ef847ede09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
