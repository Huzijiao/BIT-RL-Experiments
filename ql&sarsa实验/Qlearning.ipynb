{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np \n",
    "import torch\n",
    "# import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "    def __init__(self, obs_n, act_n, learning_rate=0.01, gamma=0.9, e_greed=0.1):\n",
    "        self.act_n = act_n      # 动作维度，有几个动作可选\n",
    "        self.lr = learning_rate # 学习率\n",
    "        self.gamma = gamma      # reward的衰减率\n",
    "        self.epsilon = e_greed  # 按一定概率随机选动作\n",
    "        self.Q = np.zeros((obs_n, act_n))\n",
    "        self.algo_name = 'Q-learning'  # 算法名称，我们使用Q学习算法\n",
    "        self.env_name = 'CliffWalking-v0'  # 环境名称，悬崖行走\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 检测GPU，如果没装CUDA的话默认为CPU\n",
    "        self.sample_count = 0\n",
    "\n",
    "    # 智能体选择动作(带探索)\n",
    "    def sample(self, obs):\n",
    "        # e-greedy 策略\n",
    "        if np.random.uniform(0, 1) < (1.0 - self.epsilon): \n",
    "            #根据table的Q值选择动作\n",
    "            action = self.predict(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.act_n) \n",
    "            #有一定概率随机探索选取一个动作\n",
    "        return action\n",
    "\n",
    "    # 根据输入观察值，依据Q-table预测输出的动作值\n",
    "    def predict(self, obs):\n",
    "        Q_list = self.Q[obs, :]\n",
    "        # 选择Q(s,a)最大对应的动作\n",
    "        maxQ = np.max(Q_list)\n",
    "        action_list = np.where(Q_list == maxQ)[0]  \n",
    "        # maxQ可能对应多个action\n",
    "        action = np.random.choice(action_list)\n",
    "        return action\n",
    "\n",
    "    # Q学习算法更新Q表格的方法\n",
    "    def learn(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\" 采用off-policy\n",
    "            obs: 交互前的状态\n",
    "            action: 本次交互选择的action\n",
    "            reward: 本次动作获得的奖励\n",
    "            next_obs: 本次交互后的状态\n",
    "            done: 布尔值,episode是否结束\n",
    "        \"\"\"\n",
    "        \n",
    "        predict_Q = self.Q[obs, action]  # 读取预测价值\n",
    "        if done: # 终止状态判断\n",
    "            target_Q = reward  # 终止状态下获取不到下一个动作，直接将 Q_target 更新为对应的奖励\n",
    "        else:\n",
    "            target_Q = reward + self.gamma * np.max(self.Q[next_obs, :]) # Q-learning\n",
    "        self.Q[obs, action] += self.lr * (target_Q - predict_Q) # 修正q\n",
    "\n",
    "    # 把Q表格的数据保存到文件中\n",
    "    def save(self):\n",
    "        npy_file = './q_table.npy'\n",
    "        np.save(npy_file, self.Q)\n",
    "        print(npy_file + ' saved.')\n",
    "\n",
    "    # 从文件中读取数据到Q表格\n",
    "    def restore(self, npy_file='./q_table.npy'):\n",
    "        self.Q = np.load(npy_file)\n",
    "        print(npy_file + ' loaded.')\n",
    "\n",
    "    # def save(self, path):\n",
    "    #     torch.save(\n",
    "    #         obj=self.Q_table,\n",
    "    #         f=path + \"Qlearning_model.pkl\",\n",
    "    #         pickle_module=dill\n",
    "    #     )\n",
    "    #     print(\"保存模型成功！\")\n",
    " \n",
    "    # def load(self, path)\n",
    "    # self.Q= torch.load(f=path + 'Qlearning_model.pkl', pickle_module=dill)\n",
    "    # print(\"加载模型成功！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train.py 一个回合\n",
    "def run_episode(env, agent, render=False):\n",
    "    # print('开始一个回合的训练')\n",
    "    # print(f'环境:{agent.env_name}, 算法:{agent.algo_name}, 设备:{agent.device}')\n",
    "    total_steps = 0 # 记录每个回合走了多少step\n",
    "    total_reward = 0  # 记录每个回合的全部奖励\n",
    "    rewards = []  # 记录每回合的奖励，用来记录并分析奖励的变化\n",
    "    ma_rewards = []  # 由于得到的奖励可能会产生振荡，使用一个滑动平均的量来反映奖励变化的趋势\n",
    "    obs = env.reset() # 重置环境, 重新开一个回合（即开始新的一个episode）\n",
    "    # 开始当前回合的行走，直至走到终点\n",
    "    while True:\n",
    "        action = agent.sample(obs)  # 根据算法选择一个动作\n",
    "        next_obs, reward, done, _ = env.step(action) # 与环境进行一次动作交互\n",
    "        # 训练Q-learning算法，更新Q-table\n",
    "        agent.learn(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs  # 更新状态\n",
    "        total_reward += reward\n",
    "        total_steps += 1 # 计算step数\n",
    "        if render:\n",
    "            env.render() #渲染新的一帧图形,返回一个图像\n",
    "        if done:\n",
    "            break\n",
    "    if ma_rewards:  # 用新的奖励与上一个奖励计算出一个平均的奖励加入到列表中，反映奖励变化的趋势\n",
    "            ma_rewards.append(ma_rewards[-1] * 0.9 + total_reward * 0.1)\n",
    "    else:\n",
    "            ma_rewards.append(total_reward)\n",
    "    return total_reward, total_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试与训练的方法基本一致，唯一的区别只是不用再进行 Q表格的更新\n",
    "def test_episode(env, agent):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = agent.predict(obs) # greedy\n",
    "        #推进一个时间步长，返回observation为对环境的一次观察，reward奖励，done代表是否要重置环境，info用于调试的诊断信息\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        time.sleep(0.5)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: steps = 160 , reward = -754.0\n",
      "Episode 1: steps = 943 , reward = -1834.0\n",
      "Episode 2: steps = 110 , reward = -110.0\n",
      "Episode 3: steps = 180 , reward = -279.0\n",
      "Episode 4: steps = 412 , reward = -808.0\n",
      "Episode 5: steps = 414 , reward = -1305.0\n",
      "Episode 6: steps = 112 , reward = -310.0\n",
      "Episode 7: steps = 145 , reward = -145.0\n",
      "Episode 8: steps = 204 , reward = -501.0\n",
      "Episode 9: steps = 137 , reward = -137.0\n",
      "Episode 10: steps = 117 , reward = -315.0\n",
      "Episode 11: steps = 147 , reward = -147.0\n",
      "Episode 12: steps = 66 , reward = -66.0\n",
      "Episode 13: steps = 108 , reward = -108.0\n",
      "Episode 14: steps = 86 , reward = -86.0\n",
      "Episode 15: steps = 122 , reward = -122.0\n",
      "Episode 16: steps = 103 , reward = -301.0\n",
      "Episode 17: steps = 175 , reward = -274.0\n",
      "Episode 18: steps = 60 , reward = -60.0\n",
      "Episode 19: steps = 84 , reward = -84.0\n",
      "Episode 20: steps = 53 , reward = -53.0\n",
      "Episode 21: steps = 93 , reward = -93.0\n",
      "Episode 22: steps = 212 , reward = -509.0\n",
      "Episode 23: steps = 49 , reward = -49.0\n",
      "Episode 24: steps = 108 , reward = -108.0\n",
      "Episode 25: steps = 30 , reward = -30.0\n",
      "Episode 26: steps = 125 , reward = -224.0\n",
      "Episode 27: steps = 84 , reward = -84.0\n",
      "Episode 28: steps = 34 , reward = -34.0\n",
      "Episode 29: steps = 49 , reward = -148.0\n",
      "Episode 30: steps = 60 , reward = -60.0\n",
      "Episode 31: steps = 119 , reward = -218.0\n",
      "Episode 32: steps = 77 , reward = -77.0\n",
      "Episode 33: steps = 156 , reward = -453.0\n",
      "Episode 34: steps = 66 , reward = -165.0\n",
      "Episode 35: steps = 63 , reward = -63.0\n",
      "Episode 36: steps = 47 , reward = -47.0\n",
      "Episode 37: steps = 113 , reward = -212.0\n",
      "Episode 38: steps = 26 , reward = -26.0\n",
      "Episode 39: steps = 111 , reward = -210.0\n",
      "Episode 40: steps = 87 , reward = -87.0\n",
      "Episode 41: steps = 66 , reward = -165.0\n",
      "Episode 42: steps = 120 , reward = -318.0\n",
      "Episode 43: steps = 76 , reward = -175.0\n",
      "Episode 44: steps = 71 , reward = -170.0\n",
      "Episode 45: steps = 100 , reward = -199.0\n",
      "Episode 46: steps = 41 , reward = -41.0\n",
      "Episode 47: steps = 41 , reward = -41.0\n",
      "Episode 48: steps = 36 , reward = -36.0\n",
      "Episode 49: steps = 88 , reward = -88.0\n",
      "Episode 50: steps = 37 , reward = -37.0\n",
      "Episode 51: steps = 98 , reward = -197.0\n",
      "Episode 52: steps = 50 , reward = -50.0\n",
      "Episode 53: steps = 77 , reward = -77.0\n",
      "Episode 54: steps = 40 , reward = -40.0\n",
      "Episode 55: steps = 39 , reward = -39.0\n",
      "Episode 56: steps = 36 , reward = -36.0\n",
      "Episode 57: steps = 70 , reward = -169.0\n",
      "Episode 58: steps = 57 , reward = -57.0\n",
      "Episode 59: steps = 61 , reward = -160.0\n",
      "Episode 60: steps = 83 , reward = -182.0\n",
      "Episode 61: steps = 38 , reward = -236.0\n",
      "Episode 62: steps = 87 , reward = -186.0\n",
      "Episode 63: steps = 67 , reward = -67.0\n",
      "Episode 64: steps = 44 , reward = -44.0\n",
      "Episode 65: steps = 46 , reward = -46.0\n",
      "Episode 66: steps = 33 , reward = -33.0\n",
      "Episode 67: steps = 21 , reward = -21.0\n",
      "Episode 68: steps = 62 , reward = -62.0\n",
      "Episode 69: steps = 60 , reward = -159.0\n",
      "Episode 70: steps = 60 , reward = -60.0\n",
      "Episode 71: steps = 31 , reward = -31.0\n",
      "Episode 72: steps = 40 , reward = -40.0\n",
      "Episode 73: steps = 44 , reward = -143.0\n",
      "Episode 74: steps = 70 , reward = -367.0\n",
      "Episode 75: steps = 58 , reward = -58.0\n",
      "Episode 76: steps = 34 , reward = -34.0\n",
      "Episode 77: steps = 40 , reward = -40.0\n",
      "Episode 78: steps = 38 , reward = -38.0\n",
      "Episode 79: steps = 32 , reward = -32.0\n",
      "Episode 80: steps = 75 , reward = -273.0\n",
      "Episode 81: steps = 27 , reward = -27.0\n",
      "Episode 82: steps = 39 , reward = -39.0\n",
      "Episode 83: steps = 43 , reward = -241.0\n",
      "Episode 84: steps = 48 , reward = -147.0\n",
      "Episode 85: steps = 33 , reward = -33.0\n",
      "Episode 86: steps = 44 , reward = -44.0\n",
      "Episode 87: steps = 32 , reward = -32.0\n",
      "Episode 88: steps = 54 , reward = -252.0\n",
      "Episode 89: steps = 45 , reward = -45.0\n",
      "Episode 90: steps = 29 , reward = -29.0\n",
      "Episode 91: steps = 48 , reward = -48.0\n",
      "Episode 92: steps = 30 , reward = -129.0\n",
      "Episode 93: steps = 23 , reward = -23.0\n",
      "Episode 94: steps = 44 , reward = -44.0\n",
      "Episode 95: steps = 27 , reward = -126.0\n",
      "Episode 96: steps = 29 , reward = -29.0\n",
      "Episode 97: steps = 58 , reward = -58.0\n",
      "Episode 98: steps = 42 , reward = -141.0\n",
      "Episode 99: steps = 36 , reward = -36.0\n",
      "Episode 100: steps = 28 , reward = -28.0\n",
      "Episode 101: steps = 41 , reward = -41.0\n",
      "Episode 102: steps = 25 , reward = -25.0\n",
      "Episode 103: steps = 47 , reward = -47.0\n",
      "Episode 104: steps = 53 , reward = -152.0\n",
      "Episode 105: steps = 30 , reward = -129.0\n",
      "Episode 106: steps = 32 , reward = -32.0\n",
      "Episode 107: steps = 29 , reward = -128.0\n",
      "Episode 108: steps = 43 , reward = -43.0\n",
      "Episode 109: steps = 59 , reward = -158.0\n",
      "Episode 110: steps = 50 , reward = -50.0\n",
      "Episode 111: steps = 20 , reward = -20.0\n",
      "Episode 112: steps = 35 , reward = -35.0\n",
      "Episode 113: steps = 28 , reward = -28.0\n",
      "Episode 114: steps = 48 , reward = -48.0\n",
      "Episode 115: steps = 34 , reward = -34.0\n",
      "Episode 116: steps = 24 , reward = -24.0\n",
      "Episode 117: steps = 41 , reward = -41.0\n",
      "Episode 118: steps = 18 , reward = -18.0\n",
      "Episode 119: steps = 43 , reward = -43.0\n",
      "Episode 120: steps = 48 , reward = -147.0\n",
      "Episode 121: steps = 28 , reward = -28.0\n",
      "Episode 122: steps = 33 , reward = -33.0\n",
      "Episode 123: steps = 25 , reward = -25.0\n",
      "Episode 124: steps = 34 , reward = -34.0\n",
      "Episode 125: steps = 20 , reward = -20.0\n",
      "Episode 126: steps = 33 , reward = -33.0\n",
      "Episode 127: steps = 27 , reward = -27.0\n",
      "Episode 128: steps = 39 , reward = -39.0\n",
      "Episode 129: steps = 31 , reward = -31.0\n",
      "Episode 130: steps = 29 , reward = -29.0\n",
      "Episode 131: steps = 15 , reward = -15.0\n",
      "Episode 132: steps = 41 , reward = -41.0\n",
      "Episode 133: steps = 30 , reward = -30.0\n",
      "Episode 134: steps = 45 , reward = -144.0\n",
      "Episode 135: steps = 22 , reward = -22.0\n",
      "Episode 136: steps = 41 , reward = -41.0\n",
      "Episode 137: steps = 25 , reward = -124.0\n",
      "Episode 138: steps = 32 , reward = -32.0\n",
      "Episode 139: steps = 22 , reward = -22.0\n",
      "Episode 140: steps = 22 , reward = -22.0\n",
      "Episode 141: steps = 45 , reward = -45.0\n",
      "Episode 142: steps = 44 , reward = -143.0\n",
      "Episode 143: steps = 26 , reward = -125.0\n",
      "Episode 144: steps = 41 , reward = -140.0\n",
      "Episode 145: steps = 38 , reward = -38.0\n",
      "Episode 146: steps = 36 , reward = -36.0\n",
      "Episode 147: steps = 51 , reward = -249.0\n",
      "Episode 148: steps = 21 , reward = -21.0\n",
      "Episode 149: steps = 20 , reward = -20.0\n",
      "Episode 150: steps = 21 , reward = -21.0\n",
      "Episode 151: steps = 19 , reward = -19.0\n",
      "Episode 152: steps = 33 , reward = -33.0\n",
      "Episode 153: steps = 29 , reward = -29.0\n",
      "Episode 154: steps = 26 , reward = -125.0\n",
      "Episode 155: steps = 41 , reward = -239.0\n",
      "Episode 156: steps = 35 , reward = -35.0\n",
      "Episode 157: steps = 29 , reward = -29.0\n",
      "Episode 158: steps = 36 , reward = -135.0\n",
      "Episode 159: steps = 46 , reward = -145.0\n",
      "Episode 160: steps = 24 , reward = -24.0\n",
      "Episode 161: steps = 53 , reward = -152.0\n",
      "Episode 162: steps = 19 , reward = -118.0\n",
      "Episode 163: steps = 18 , reward = -18.0\n",
      "Episode 164: steps = 20 , reward = -20.0\n",
      "Episode 165: steps = 32 , reward = -230.0\n",
      "Episode 166: steps = 17 , reward = -17.0\n",
      "Episode 167: steps = 44 , reward = -44.0\n",
      "Episode 168: steps = 21 , reward = -21.0\n",
      "Episode 169: steps = 72 , reward = -369.0\n",
      "Episode 170: steps = 15 , reward = -15.0\n",
      "Episode 171: steps = 39 , reward = -237.0\n",
      "Episode 172: steps = 20 , reward = -20.0\n",
      "Episode 173: steps = 47 , reward = -146.0\n",
      "Episode 174: steps = 36 , reward = -36.0\n",
      "Episode 175: steps = 29 , reward = -29.0\n",
      "Episode 176: steps = 13 , reward = -13.0\n",
      "Episode 177: steps = 22 , reward = -22.0\n",
      "Episode 178: steps = 26 , reward = -26.0\n",
      "Episode 179: steps = 17 , reward = -17.0\n",
      "Episode 180: steps = 20 , reward = -218.0\n",
      "Episode 181: steps = 14 , reward = -14.0\n",
      "Episode 182: steps = 56 , reward = -155.0\n",
      "Episode 183: steps = 30 , reward = -129.0\n",
      "Episode 184: steps = 57 , reward = -354.0\n",
      "Episode 185: steps = 23 , reward = -23.0\n",
      "Episode 186: steps = 35 , reward = -134.0\n",
      "Episode 187: steps = 20 , reward = -20.0\n",
      "Episode 188: steps = 35 , reward = -35.0\n",
      "Episode 189: steps = 19 , reward = -19.0\n",
      "Episode 190: steps = 21 , reward = -120.0\n",
      "Episode 191: steps = 30 , reward = -30.0\n",
      "Episode 192: steps = 18 , reward = -18.0\n",
      "Episode 193: steps = 30 , reward = -30.0\n",
      "Episode 194: steps = 15 , reward = -15.0\n",
      "Episode 195: steps = 16 , reward = -16.0\n",
      "Episode 196: steps = 33 , reward = -132.0\n",
      "Episode 197: steps = 37 , reward = -136.0\n",
      "Episode 198: steps = 19 , reward = -19.0\n",
      "Episode 199: steps = 19 , reward = -19.0\n",
      "Episode 200: steps = 27 , reward = -27.0\n",
      "Episode 201: steps = 21 , reward = -21.0\n",
      "Episode 202: steps = 14 , reward = -14.0\n",
      "Episode 203: steps = 38 , reward = -137.0\n",
      "Episode 204: steps = 13 , reward = -13.0\n",
      "Episode 205: steps = 14 , reward = -14.0\n",
      "Episode 206: steps = 17 , reward = -17.0\n",
      "Episode 207: steps = 17 , reward = -17.0\n",
      "Episode 208: steps = 34 , reward = -34.0\n",
      "Episode 209: steps = 37 , reward = -235.0\n",
      "Episode 210: steps = 20 , reward = -20.0\n",
      "Episode 211: steps = 15 , reward = -15.0\n",
      "Episode 212: steps = 24 , reward = -24.0\n",
      "Episode 213: steps = 20 , reward = -20.0\n",
      "Episode 214: steps = 15 , reward = -15.0\n",
      "Episode 215: steps = 21 , reward = -21.0\n",
      "Episode 216: steps = 15 , reward = -15.0\n",
      "Episode 217: steps = 15 , reward = -15.0\n",
      "Episode 218: steps = 27 , reward = -225.0\n",
      "Episode 219: steps = 27 , reward = -27.0\n",
      "Episode 220: steps = 40 , reward = -139.0\n",
      "Episode 221: steps = 15 , reward = -15.0\n",
      "Episode 222: steps = 17 , reward = -17.0\n",
      "Episode 223: steps = 27 , reward = -27.0\n",
      "Episode 224: steps = 23 , reward = -122.0\n",
      "Episode 225: steps = 15 , reward = -15.0\n",
      "Episode 226: steps = 16 , reward = -115.0\n",
      "Episode 227: steps = 20 , reward = -20.0\n",
      "Episode 228: steps = 15 , reward = -15.0\n",
      "Episode 229: steps = 31 , reward = -31.0\n",
      "Episode 230: steps = 21 , reward = -120.0\n",
      "Episode 231: steps = 15 , reward = -15.0\n",
      "Episode 232: steps = 15 , reward = -15.0\n",
      "Episode 233: steps = 23 , reward = -23.0\n",
      "Episode 234: steps = 23 , reward = -23.0\n",
      "Episode 235: steps = 17 , reward = -116.0\n",
      "Episode 236: steps = 26 , reward = -26.0\n",
      "Episode 237: steps = 13 , reward = -13.0\n",
      "Episode 238: steps = 35 , reward = -134.0\n",
      "Episode 239: steps = 15 , reward = -15.0\n",
      "Episode 240: steps = 17 , reward = -17.0\n",
      "Episode 241: steps = 23 , reward = -23.0\n",
      "Episode 242: steps = 14 , reward = -14.0\n",
      "Episode 243: steps = 13 , reward = -13.0\n",
      "Episode 244: steps = 14 , reward = -14.0\n",
      "Episode 245: steps = 28 , reward = -28.0\n",
      "Episode 246: steps = 19 , reward = -19.0\n",
      "Episode 247: steps = 26 , reward = -224.0\n",
      "Episode 248: steps = 28 , reward = -325.0\n",
      "Episode 249: steps = 15 , reward = -15.0\n",
      "Episode 250: steps = 21 , reward = -120.0\n",
      "Episode 251: steps = 28 , reward = -28.0\n",
      "Episode 252: steps = 15 , reward = -15.0\n",
      "Episode 253: steps = 13 , reward = -13.0\n",
      "Episode 254: steps = 15 , reward = -15.0\n",
      "Episode 255: steps = 19 , reward = -19.0\n",
      "Episode 256: steps = 13 , reward = -13.0\n",
      "Episode 257: steps = 15 , reward = -15.0\n",
      "Episode 258: steps = 32 , reward = -230.0\n",
      "Episode 259: steps = 13 , reward = -13.0\n",
      "Episode 260: steps = 13 , reward = -13.0\n",
      "Episode 261: steps = 25 , reward = -124.0\n",
      "Episode 262: steps = 14 , reward = -14.0\n",
      "Episode 263: steps = 18 , reward = -18.0\n",
      "Episode 264: steps = 45 , reward = -144.0\n",
      "Episode 265: steps = 15 , reward = -15.0\n",
      "Episode 266: steps = 13 , reward = -13.0\n",
      "Episode 267: steps = 15 , reward = -15.0\n",
      "Episode 268: steps = 19 , reward = -19.0\n",
      "Episode 269: steps = 15 , reward = -114.0\n",
      "Episode 270: steps = 13 , reward = -13.0\n",
      "Episode 271: steps = 14 , reward = -14.0\n",
      "Episode 272: steps = 15 , reward = -15.0\n",
      "Episode 273: steps = 20 , reward = -20.0\n",
      "Episode 274: steps = 14 , reward = -14.0\n",
      "Episode 275: steps = 13 , reward = -13.0\n",
      "Episode 276: steps = 13 , reward = -13.0\n",
      "Episode 277: steps = 13 , reward = -13.0\n",
      "Episode 278: steps = 15 , reward = -15.0\n",
      "Episode 279: steps = 37 , reward = -136.0\n",
      "Episode 280: steps = 15 , reward = -15.0\n",
      "Episode 281: steps = 17 , reward = -17.0\n",
      "Episode 282: steps = 16 , reward = -16.0\n",
      "Episode 283: steps = 15 , reward = -15.0\n",
      "Episode 284: steps = 13 , reward = -13.0\n",
      "Episode 285: steps = 13 , reward = -13.0\n",
      "Episode 286: steps = 13 , reward = -13.0\n",
      "Episode 287: steps = 25 , reward = -25.0\n",
      "Episode 288: steps = 35 , reward = -233.0\n",
      "Episode 289: steps = 13 , reward = -13.0\n",
      "Episode 290: steps = 19 , reward = -19.0\n",
      "Episode 291: steps = 13 , reward = -13.0\n",
      "Episode 292: steps = 13 , reward = -13.0\n",
      "Episode 293: steps = 21 , reward = -21.0\n",
      "Episode 294: steps = 13 , reward = -13.0\n",
      "Episode 295: steps = 17 , reward = -17.0\n",
      "Episode 296: steps = 16 , reward = -115.0\n",
      "Episode 297: steps = 21 , reward = -21.0\n",
      "Episode 298: steps = 23 , reward = -23.0\n",
      "Episode 299: steps = 15 , reward = -15.0\n",
      "Episode 300: steps = 17 , reward = -17.0\n",
      "Episode 301: steps = 15 , reward = -15.0\n",
      "Episode 302: steps = 29 , reward = -128.0\n",
      "Episode 303: steps = 16 , reward = -16.0\n",
      "Episode 304: steps = 22 , reward = -121.0\n",
      "Episode 305: steps = 17 , reward = -17.0\n",
      "Episode 306: steps = 13 , reward = -13.0\n",
      "Episode 307: steps = 21 , reward = -120.0\n",
      "Episode 308: steps = 20 , reward = -20.0\n",
      "Episode 309: steps = 19 , reward = -19.0\n",
      "Episode 310: steps = 13 , reward = -13.0\n",
      "Episode 311: steps = 13 , reward = -13.0\n",
      "Episode 312: steps = 20 , reward = -20.0\n",
      "Episode 313: steps = 13 , reward = -13.0\n",
      "Episode 314: steps = 13 , reward = -13.0\n",
      "Episode 315: steps = 22 , reward = -121.0\n",
      "Episode 316: steps = 13 , reward = -13.0\n",
      "Episode 317: steps = 21 , reward = -120.0\n",
      "Episode 318: steps = 17 , reward = -17.0\n",
      "Episode 319: steps = 14 , reward = -113.0\n",
      "Episode 320: steps = 26 , reward = -125.0\n",
      "Episode 321: steps = 21 , reward = -120.0\n",
      "Episode 322: steps = 15 , reward = -15.0\n",
      "Episode 323: steps = 13 , reward = -13.0\n",
      "Episode 324: steps = 13 , reward = -13.0\n",
      "Episode 325: steps = 15 , reward = -15.0\n",
      "Episode 326: steps = 14 , reward = -14.0\n",
      "Episode 327: steps = 13 , reward = -13.0\n",
      "Episode 328: steps = 18 , reward = -18.0\n",
      "Episode 329: steps = 13 , reward = -13.0\n",
      "Episode 330: steps = 13 , reward = -13.0\n",
      "Episode 331: steps = 15 , reward = -15.0\n",
      "Episode 332: steps = 22 , reward = -121.0\n",
      "Episode 333: steps = 13 , reward = -13.0\n",
      "Episode 334: steps = 24 , reward = -123.0\n",
      "Episode 335: steps = 15 , reward = -15.0\n",
      "Episode 336: steps = 13 , reward = -13.0\n",
      "Episode 337: steps = 13 , reward = -13.0\n",
      "Episode 338: steps = 13 , reward = -13.0\n",
      "Episode 339: steps = 21 , reward = -21.0\n",
      "Episode 340: steps = 13 , reward = -13.0\n",
      "Episode 341: steps = 25 , reward = -124.0\n",
      "Episode 342: steps = 15 , reward = -15.0\n",
      "Episode 343: steps = 17 , reward = -215.0\n",
      "Episode 344: steps = 13 , reward = -13.0\n",
      "Episode 345: steps = 13 , reward = -13.0\n",
      "Episode 346: steps = 13 , reward = -13.0\n",
      "Episode 347: steps = 15 , reward = -15.0\n",
      "Episode 348: steps = 17 , reward = -17.0\n",
      "Episode 349: steps = 15 , reward = -15.0\n",
      "Episode 350: steps = 23 , reward = -23.0\n",
      "Episode 351: steps = 13 , reward = -13.0\n",
      "Episode 352: steps = 17 , reward = -17.0\n",
      "Episode 353: steps = 38 , reward = -335.0\n",
      "Episode 354: steps = 26 , reward = -125.0\n",
      "Episode 355: steps = 35 , reward = -233.0\n",
      "Episode 356: steps = 13 , reward = -13.0\n",
      "Episode 357: steps = 13 , reward = -13.0\n",
      "Episode 358: steps = 13 , reward = -13.0\n",
      "Episode 359: steps = 33 , reward = -231.0\n",
      "Episode 360: steps = 24 , reward = -24.0\n",
      "Episode 361: steps = 24 , reward = -123.0\n",
      "Episode 362: steps = 17 , reward = -17.0\n",
      "Episode 363: steps = 19 , reward = -19.0\n",
      "Episode 364: steps = 13 , reward = -13.0\n",
      "Episode 365: steps = 14 , reward = -14.0\n",
      "Episode 366: steps = 14 , reward = -14.0\n",
      "Episode 367: steps = 18 , reward = -117.0\n",
      "Episode 368: steps = 15 , reward = -15.0\n",
      "Episode 369: steps = 13 , reward = -13.0\n",
      "Episode 370: steps = 13 , reward = -13.0\n",
      "Episode 371: steps = 15 , reward = -15.0\n",
      "Episode 372: steps = 30 , reward = -228.0\n",
      "Episode 373: steps = 13 , reward = -13.0\n",
      "Episode 374: steps = 13 , reward = -13.0\n",
      "Episode 375: steps = 18 , reward = -117.0\n",
      "Episode 376: steps = 13 , reward = -13.0\n",
      "Episode 377: steps = 13 , reward = -13.0\n",
      "Episode 378: steps = 19 , reward = -19.0\n",
      "Episode 379: steps = 15 , reward = -15.0\n",
      "Episode 380: steps = 13 , reward = -13.0\n",
      "Episode 381: steps = 25 , reward = -124.0\n",
      "Episode 382: steps = 14 , reward = -14.0\n",
      "Episode 383: steps = 19 , reward = -19.0\n",
      "Episode 384: steps = 13 , reward = -13.0\n",
      "Episode 385: steps = 13 , reward = -13.0\n",
      "Episode 386: steps = 17 , reward = -17.0\n",
      "Episode 387: steps = 13 , reward = -13.0\n",
      "Episode 388: steps = 13 , reward = -13.0\n",
      "Episode 389: steps = 29 , reward = -227.0\n",
      "Episode 390: steps = 13 , reward = -13.0\n",
      "Episode 391: steps = 23 , reward = -23.0\n",
      "Episode 392: steps = 13 , reward = -13.0\n",
      "Episode 393: steps = 13 , reward = -13.0\n",
      "Episode 394: steps = 16 , reward = -16.0\n",
      "Episode 395: steps = 13 , reward = -13.0\n",
      "Episode 396: steps = 15 , reward = -15.0\n",
      "Episode 397: steps = 15 , reward = -15.0\n",
      "Episode 398: steps = 27 , reward = -225.0\n",
      "Episode 399: steps = 13 , reward = -13.0\n",
      "Episode 400: steps = 13 , reward = -13.0\n",
      "Episode 401: steps = 13 , reward = -13.0\n",
      "Episode 402: steps = 13 , reward = -13.0\n",
      "Episode 403: steps = 13 , reward = -13.0\n",
      "Episode 404: steps = 21 , reward = -21.0\n",
      "Episode 405: steps = 14 , reward = -14.0\n",
      "Episode 406: steps = 13 , reward = -13.0\n",
      "Episode 407: steps = 23 , reward = -122.0\n",
      "Episode 408: steps = 13 , reward = -13.0\n",
      "Episode 409: steps = 13 , reward = -13.0\n",
      "Episode 410: steps = 15 , reward = -15.0\n",
      "Episode 411: steps = 17 , reward = -17.0\n",
      "Episode 412: steps = 13 , reward = -13.0\n",
      "Episode 413: steps = 13 , reward = -13.0\n",
      "Episode 414: steps = 33 , reward = -231.0\n",
      "Episode 415: steps = 13 , reward = -13.0\n",
      "Episode 416: steps = 13 , reward = -13.0\n",
      "Episode 417: steps = 13 , reward = -13.0\n",
      "Episode 418: steps = 22 , reward = -22.0\n",
      "Episode 419: steps = 13 , reward = -13.0\n",
      "Episode 420: steps = 19 , reward = -118.0\n",
      "Episode 421: steps = 13 , reward = -13.0\n",
      "Episode 422: steps = 15 , reward = -15.0\n",
      "Episode 423: steps = 14 , reward = -14.0\n",
      "Episode 424: steps = 15 , reward = -15.0\n",
      "Episode 425: steps = 13 , reward = -13.0\n",
      "Episode 426: steps = 14 , reward = -14.0\n",
      "Episode 427: steps = 20 , reward = -119.0\n",
      "Episode 428: steps = 13 , reward = -13.0\n",
      "Episode 429: steps = 13 , reward = -13.0\n",
      "Episode 430: steps = 15 , reward = -15.0\n",
      "Episode 431: steps = 21 , reward = -21.0\n",
      "Episode 432: steps = 32 , reward = -131.0\n",
      "Episode 433: steps = 13 , reward = -13.0\n",
      "Episode 434: steps = 21 , reward = -120.0\n",
      "Episode 435: steps = 17 , reward = -17.0\n",
      "Episode 436: steps = 21 , reward = -120.0\n",
      "Episode 437: steps = 16 , reward = -115.0\n",
      "Episode 438: steps = 15 , reward = -15.0\n",
      "Episode 439: steps = 28 , reward = -127.0\n",
      "Episode 440: steps = 13 , reward = -13.0\n",
      "Episode 441: steps = 14 , reward = -14.0\n",
      "Episode 442: steps = 15 , reward = -15.0\n",
      "Episode 443: steps = 13 , reward = -13.0\n",
      "Episode 444: steps = 13 , reward = -13.0\n",
      "Episode 445: steps = 13 , reward = -13.0\n",
      "Episode 446: steps = 15 , reward = -15.0\n",
      "Episode 447: steps = 15 , reward = -15.0\n",
      "Episode 448: steps = 19 , reward = -19.0\n",
      "Episode 449: steps = 27 , reward = -126.0\n",
      "Episode 450: steps = 14 , reward = -14.0\n",
      "Episode 451: steps = 14 , reward = -14.0\n",
      "Episode 452: steps = 24 , reward = -123.0\n",
      "Episode 453: steps = 14 , reward = -14.0\n",
      "Episode 454: steps = 13 , reward = -13.0\n",
      "Episode 455: steps = 15 , reward = -15.0\n",
      "Episode 456: steps = 15 , reward = -15.0\n",
      "Episode 457: steps = 16 , reward = -115.0\n",
      "Episode 458: steps = 18 , reward = -117.0\n",
      "Episode 459: steps = 13 , reward = -13.0\n",
      "Episode 460: steps = 15 , reward = -15.0\n",
      "Episode 461: steps = 13 , reward = -13.0\n",
      "Episode 462: steps = 21 , reward = -120.0\n",
      "Episode 463: steps = 13 , reward = -13.0\n",
      "Episode 464: steps = 13 , reward = -13.0\n",
      "Episode 465: steps = 16 , reward = -16.0\n",
      "Episode 466: steps = 19 , reward = -118.0\n",
      "Episode 467: steps = 24 , reward = -24.0\n",
      "Episode 468: steps = 15 , reward = -15.0\n",
      "Episode 469: steps = 14 , reward = -14.0\n",
      "Episode 470: steps = 15 , reward = -15.0\n",
      "Episode 471: steps = 15 , reward = -15.0\n",
      "Episode 472: steps = 41 , reward = -437.0\n",
      "Episode 473: steps = 20 , reward = -20.0\n",
      "Episode 474: steps = 21 , reward = -120.0\n",
      "Episode 475: steps = 13 , reward = -13.0\n",
      "Episode 476: steps = 15 , reward = -15.0\n",
      "Episode 477: steps = 13 , reward = -13.0\n",
      "Episode 478: steps = 13 , reward = -13.0\n",
      "Episode 479: steps = 24 , reward = -123.0\n",
      "Episode 480: steps = 13 , reward = -13.0\n",
      "Episode 481: steps = 13 , reward = -13.0\n",
      "Episode 482: steps = 15 , reward = -15.0\n",
      "Episode 483: steps = 19 , reward = -19.0\n",
      "Episode 484: steps = 17 , reward = -116.0\n",
      "Episode 485: steps = 13 , reward = -13.0\n",
      "Episode 486: steps = 13 , reward = -13.0\n",
      "Episode 487: steps = 13 , reward = -13.0\n",
      "Episode 488: steps = 32 , reward = -230.0\n",
      "Episode 489: steps = 15 , reward = -15.0\n",
      "Episode 490: steps = 18 , reward = -18.0\n",
      "Episode 491: steps = 15 , reward = -15.0\n",
      "Episode 492: steps = 13 , reward = -13.0\n",
      "Episode 493: steps = 15 , reward = -15.0\n",
      "Episode 494: steps = 13 , reward = -13.0\n",
      "Episode 495: steps = 27 , reward = -126.0\n",
      "Episode 496: steps = 15 , reward = -15.0\n",
      "Episode 497: steps = 14 , reward = -14.0\n",
      "Episode 498: steps = 17 , reward = -17.0\n",
      "Episode 499: steps = 15 , reward = -15.0\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  x  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  x  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  x  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  x  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  x\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "test reward = -13.0\n"
     ]
    }
   ],
   "source": [
    "# 使用gym创建悬崖环境\n",
    "env = gym.make(\"CliffWalking-v0\")  # 0 up, 1 right, 2 down, 3 left\n",
    "\n",
    "# 创建一个agent实例，输入超参数(Hyperparameter),tuning parameters需要人为设定\n",
    "agent = QLearningAgent(\n",
    "    obs_n=env.observation_space.n,  # 状态维度，即4*12的网格中的 48 个状态\n",
    "    act_n=env.action_space.n,  # 动作维度， 即 4 个动作\n",
    "    learning_rate=0.1,  # 学习率\n",
    "    gamma=0.9,  # 折扣因子\n",
    "    e_greed=0.1)  # ε-贪心策略中的终止epsilon，越小学习结果越逼近\n",
    "\n",
    "# 训练500个episode，打印每个episode的分数\n",
    "for episode in range(500):\n",
    "    ep_reward, ep_steps = run_episode(env, agent, False)\n",
    "    print('Episode %s: steps = %s , reward = %.1f' % (episode, ep_steps, ep_reward))\n",
    "\n",
    "# agent.save(path='./')  # 保存模型\n",
    "\n",
    "# 全部训练结束，查看算法效果\n",
    "test_reward = test_episode(env, agent)\n",
    "print('test reward = %.1f' % (test_reward))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a3f677a77b69d4794d31f44cefe23a57cb8094311290e7ebc8599ef847ede09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
